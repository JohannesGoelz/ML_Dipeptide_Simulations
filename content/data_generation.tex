
\section{Data Generation}
\label{sec:data_generation}

A central component of the project is the generation of large-scale simulation data that can serve as training input for \gls{ml} models. In these simulations, solutes are solvated in a water box, equilibrated, and then propagated over time to obtain representative trajectories. Both coordinates and forces are stored, which allows the resulting datasets to be used in diverse training settings such as force-matching.

To improve sampling efficiency, \gls{remd} \cite{qi2018replica} is incorporated. By running simulations at multiple temperatures and exchanging configurations, \gls{remd} helps to explore conformational space. This is important in order to avoid biased datasets that overrepresent only a limited set of conformations.


The data pipeline is designed for large-scale simulation of amino acid sequences. 

The pipeline begins with the generation of three-dimensional solute structures from amino acid sequences using AmberTools \texttt{tleap}, which produces capped or uncapped PDB files depending on the desired terminus configuration. 
These structures are subsequently solvated in a TIP3P explicit water box using OpenMM's \texttt{Modeller}, with a padding around the solute to ensure a sufficiently large solvation shell. The AMBER14 force field is applied to parameterize all interactions. Prior to production, each system undergoes energy minimization followed by an NPT equilibration phase, during which the Monte Carlo barostat maintains a constant pressure to relax the solvation shell and achieve an equilibrium density. The equilibrated system is then handed to the Reform-based \gls{remd} framework, which initializes one replica per temperature along an automatically determined temperature ladder, calibrated to yield an exchange acceptance rate of approximately 30\%. All replicas are propagated simultaneously using Langevin dynamics. During production, a custom \texttt{RemdForceLogger} hook intercepts coordinates and forces for all replicas at every snapshot interval and writes them in buffered chunks to a compressed HDF5 file, avoiding memory exhaustion even for long trajectories. After the simulation completes, a post-processing step extracts the frames belonging to the lowest-temperature replica, retaining only the solute atoms and discarding the surrounding solvent, and saves the resulting data as a compressed \texttt{.npz} archive containing coordinate and force arrays. The entire workflow from sequence input to job submission is orchestrated by a dedicated pipeline script that constructs SLURM batch jobs and submits them to the cluster, enabling the parallel processing of many sequences.

The scope of data generation in this project is to assemble datasets for model development and benchmarking. While the current focus lies on explicit water with standard force fields, the approach can naturally be extended to other solvents in later stages.