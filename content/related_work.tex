\section{Related Work}
\label{sec:related_work}

\citet{machine_learning_implicit_solvation} were the first to apply \gls{ml} methods to implicit solvation. Their approach employed a \gls{gnn} trained with a force-matching objective to learn solvent effects from explicit solvent simulation data. Several variants of the model were developed, differing in the type of ground-truth information employed. Specifically, the feature vectors were derived from either atom types, partial charges, or a mixture of both. Compared to the baseline \gls{gb} model, their method achieved lower \gls{kl} divergence on molecules such as chignolin and alanine dipeptide. However, the model lacked transferability across systems. It could only reproduce the Boltzmann distribution of a system for which sufficient training data were already available. While this work laid important groundwork in the field, its limited generalizability restricted its practical applicability.

Recent developments have looked forward to refine implicit solvation through machine learning. One approach is delta learning \cite{katzberger2024general, katzberger2025rapid}, which builds on classical solvation models by training a model to predict the difference between implicit and explicit solvent results. In practice, a \gls{nn} improves accuracy by modifying parameters within a \gls{gb} baseline, such as scaling the Born radii, while preserving computational efficiency. \citet{katzberger2024general} extended the model to multiple solvents, achieving speedups of about one order of magnitude compared to explicit-solvent simulations while reproducing free-energy profiles of small organic molecules. In particular, the method can recover \gls{pmf} curves of intramolecular hydrogen bonds across diverse solvents. However, the model has not been tested on molecules weighing more than 700 Da.

Another relevant approach is potential contrasting, introduced by \citet{Airas2023}. In this method, a pretrained implicit-solvation SchNet potential is refined by contrasting conformations obtained from explicit-solvent simulations with those from the implicit baseline. The model learns to discriminate realistic samples from noise-like ones, thereby eliminating the need for labeled force data. The authors trained their model on six proteins containing between 166 and 624 atoms and reported that it outperformed the GB baseline on these systems. To assess transferability, they evaluated two mutations of the protein chignolin, which was part of the training set. For the mutation with 80\% sequence identity, the model achieved reasonable accuracy, whereas for the mutation with 70\% sequence identity, the GB baseline performed better. Additionally, the authors tested the model on the intrinsically disordered peptide JIP1, which has been extensively studied by \citet{jipidp}. In this case, the SchNet model failed to capture the main energy basin observed in explicit-solvent simulations, which the GB baseline successfully reproduced. Nevertheless, the SchNet model identified three of the remaining basins, albeit with differing relative energy levels.

A further line of work was introduced by \citet{r√∂cken2025predictingsolvationfreeenergies}, who combined vacuum pre-training with solvent refinement for small, drug-like molecules. In this approach, their model ReSolv first learns a vacuum potential from \gls{qm} data using the QM7-X dataset \cite{Hoja2021QM7X} and subsequently refines it using experimental solvation free energies from the FreeSolv dataset \cite{DuarteRamosMatos2017FreeSolv}. For training, the authors identified 559 overlapping molecules (each containing up to 23 atoms) between the two databases and split them into 70\% training and 30\% test sets. The split was carefully designed to maintain consistent heavy-atom combinations and to ensure that no unseen chemical functional groups appeared in the test set. Their model consistently outperformed the CHARMM \cite{mackerell1998all} and AMBER \cite{cornell1995second} force fields with explicit water. Furthermore, they demonstrated partial transferability across chemical functional groups by retraining on modified datasets in which one functional group was omitted. Nevertheless, the observed increase in error for molecules with more than 14 heavy atoms suggests potential scaling limitations. Moreover, the primary results presented in the paper focus on in-distribution generalization due to the engineered dataset split, leaving questions about true out-of-distribution performance. Finally, a detailed analysis of whether the \gls{ml} potential can reproduce correct ensembles of molecular conformations is absent. Such an analysis is inherently important for applications like drug discovery or protein folding, where accurately capturing the Boltzmann distribution is as critical as predicting solvation free energies.
